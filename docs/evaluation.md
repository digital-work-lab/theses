---
layout: default
title: "Evaluation criteria"
has_toc: true
nav_order: 6
---

# Evaluation criteria
{: .no_toc }

Bachelor's and Master's theses are evaluated according to the same criteria.
Once the **formal requirements** are met, the **main criteria** related to the process, the contribution, and the discussion are used to grade a thesis.

A [changelog](https://github.com/digital-work-lab/theses/commits/main/docs/evaluation.md){: target="_blank"} is available for this document.

## Table of contents
{: .no_toc .text-delta }

- TOC
{:toc}

## Formal requirements

- A Bachelor's thesis should cover 30-50 pages (roughly 8,000 - 16,000 words), while a Master's thesis should cover 40-70 pages (roughly 12,000 - 28,000 words).
Design-oriented, analytical, and quantitative theses are typically shorter than qualitative and literature-based ones.
For a Master's thesis, a presentation is required, and a more substantial contribution is expected in line with the higher number of ECTS credits earned.
- Written in proper academic English, with a clear structure and line of argument
- Follows [citation practices](https://digital-work-lab.github.io/handbook/docs/20-research/20_processes/20.29.writing.html#citations-and-reference-sections){: target="_blank"}, using [APA format](https://apastyle.apa.org/style-grammar-guidelines/references/examples){: target="_blank"}, and refers primarily to peer-reviewed academic papers
- Includes a title page, table-of-contents, abstract, common sections (e.g., IMRAD), a reference section, and appendices if applicable (a list of figures or tables is not required)
- Makes appropriate use of tables and figures, citing each table or figure in the text
- Provides clear definitions for key terms

## Main criteria

The **process** is evaluated according to the following criteria:

- **Systematicity** in managing time, resources, and challenges. Start early to have enough time for each task, make sure that resources such as labs, equipment, participants, partners, or thesis advisors are available at the right time, and reduce potential risks by anticipating them and identifying potential alternatives.
- **Proactiveness** in developing the topic (research question, theory, method, etc.), and incorporating feedback. We encourage students to challenge our feedback and suggest better alternatives.
- **Proficiency** in leveraging prior research, i.e., identifying relevant papers, achieving an appropriate understanding, assessing contributions critically, organizing prior work in a persuasive way to clarify how the thesis builds on prior work and how it goes beyond prior work

The **contribution** is evaluated based on several dimensions (based on Leidner 2020):

- **Explicitness and significance of research objective or question**: Explain how your work builds on prior research, identify related contributions, and clarify the research gap you address. Readers should understand why your research promises interesting insights, and who would benefit from it. The editorial of Lange (2017) provides useful sugestions to accomplish this.
- **Uniqueness in framing prior research**: In summarizing prior research, it is essential to organize existing contributions from the perspective of your work. Prior research, in itself, does not follow a pre-defined order. It is your responsibility to make sense of the literature and to adopt an organizing frame that helps readers understand how prior research relates to your contribution.
- **Systematicity of method**: Evaluation criteria and reporting guidelines depend on the method applied. A selection is provided below. In addition to reporting the methodological steps, *data and code should be made available* with the thesis according to standards of reproducible research. Corresponding requirements should be discussed with the thesis advisor.
- **Originality of theory**: Applies if your contribution involves a theoretical contribution, e.g., in the form of propositions, constructs, or models. Clarify how it differs from related theory, and explain why it offers a plausible and more compelling perspective or explanation.
- **Interpretation and novelty of findings**: The findings should be interpreted appropriately, using a neutral tone, and avoiding to over-state or under-state their significance. Clearly distinguish findings that confirm prior research or common expectations. Effectively communicate the findings that are novel, interesting, or counterintuitive.

Recommended criteria and reporting guidelines for selected research methods (the [equator network](https://www.equator-network.org/){: target="_blank"} provides a comprehensive overview).
Additional guidelines can be provided upon request.

| Method             | Criteria and reporting guideline                                                                                                       |
|--------------------|----------------------------------------------------------------------------------------------------------------------------------------|
| Literature review  | Paré et al. 2015, Paré et al. 2023, Page et al. 2021, Templier and Paré 2018, [PRISMA](https://estech.shinyapps.io/prisma_flowdiagram/){: target="_blank"} |
| Theory development | Gregor 2006                                                                                                                             |
| Design Science     | Hevner et al. 2004, Prat et al. 2015,  Peffers et al. 2007, JOSS review criteria                                                        |
| Machine learning   | Stevens et al. 2020, Heil et al. 2021, Walsh et al. 2021, Kapoor et al. 2023                                                            |
| Experiments        | Frank et al. 2024                                                                                                                       |
| Qualitative surveys and interviews | O'Brien et al. 2014, Tong et al. 2007                                                                                   |

<!--
- Experiments (TODO)
- Surveys (TODO)

TBD: methodological coherence/fit?

Weights: We apply flexible weights because the topic of a thesis may limit the degree to which students contributions can excell in any of the contribution dimensions.
-->

The contribution should be **discussed** with reference to

- The (methodological) limitations
- The implications for future research (research gaps)
- The implications for practice (Ågerfalk and Karlsson, 2020).

## Presentation

A presentation is required for Master's theses. A presentation takes 15-20 minutes with 10 minutes of discussion. It is evaluated based on the following criteria:

- The introductory section should convey why the topic is interesting, and it should be easy to follow by a general audience
- The main section should demonstrate the objective, method, main findings, and contribution, giving experts the possibility to critically assess the different parts
- The concluding section should give an outlook and briefly outline the implications
- Presentation style and slides should be appropriate, i.e., using academic terminology, displaying a clear structure, connecting with the audience, taking approx. 2-3 minutes per slide, using short bullet-point summaries instead of longer paragraphs, including illustrations rather than animations
- Questions should be handled constructively, demonstrating in-depth knowledge of the thesis, and familiarity with the broader topic area 

In line with the applicable regulations, the presentation receives a weight of 33% for students of Information Systems. For students from other departments and degree programs (such as [IBWL](https://www.uni-bamberg.de/abt-studium/aufgaben/pruefungs-studienordnungen/masterstudiengaenge/internationale-betriebswirtschaftslehre/){: target="_blank"}), the presentation may not be graded.

<!-- 
https://www.uni-bamberg.de/psi/teaching/psi-teaching-posters/
https://www.dmm.bwl.uni-muenchen.de/download/info/dmm_formalia_2022_de.docx
-->

## References

<div class="references">
  <p>Ågerfalk, P. J., &amp; Karlsson, F. (2020). Artefactual and empirical contributions in information systems research. <em>European Journal of Information Systems</em>, 29(2), 109-113. <a href="https://www.tandfonline.com/doi/full/10.1080/0960085X.2020.1743051" target="_blank">link</a></p>
  <p>Frank, M. C., Braginsky, M., Cachia, J., Coles, N. A., Hardwicke, T. E., Hawkins, R. D., Mathur, M. B. and Williams, R. (2024). Experimentology: An Open Science Approach to Experimental Psychology Methods. MIT Press. <a href="https://experimentology.io/" target="_blank">link</a></p>
  <p>Gregor, S. (2006). The nature of theory in information systems. <em>MIS Quarterly</em>, 611-642. <a href="https://www.jstor.org/stable/25148742?casa_token=CSQ3STXqXW4AAAAA:2njIJ54TQL4NAtW49XTg8xcrQ2Bl-rufWyHmhv5ws29ZubAj9wrY9_4XPSnx1gDe06os15hw4LFJ1IVY6A-qIFCobN6WWyr4pYqbfYdMsPCsHoUTJMFp" target="_blank">link</a></p>
  <p>Heil, B. J., Hoffman, M. M., Markowetz, F., Lee, S. I., Greene, C. S., &amp; Hicks, S. C. (2021). Reproducibility standards for machine learning in the life sciences. <em>Nature Methods</em>, 18(10), 1132-1135. <a href="https://www.nature.com/articles/s41592-021-01256-7" target="_blank">Link</a></p>
  <p>Hevner, A. R., March, S. T., Park, J., &amp; Ram, S. (2008). Design science in information systems research. <em>MIS Quarterly</em>, 28(1), 6. <a href="https://aisel.aisnet.org/misq/vol28/iss1/6/" target="_blank">link</a></p>
  <p>JOSS review criteria and checklist. <a href="https://joss.readthedocs.io/en/latest/review_checklist.html" target="_blank">link</a></p>
  <p>Kapoor, S., Cantrell, E., Peng, K., Pham, T. H., Bail, C. A., Gundersen, O. E., ... &amp; Narayanan, A. (2023). Reforms: Reporting standards for machine learning based science. arXiv preprint arXiv:2308.07832.</p>
  <p>Lange, D., &amp; Pfarrer, M. D. (2017). Editors’ comments: Sense and structure—The core building blocks of an AMR article. <em>Academy of Management Review</em>, 42(3), 407-416. <a href="https://journals.aom.org/doi/full/10.5465/amr.2016.0225" target="_blank">link</a></p>
  <p>Leidner, D. E. (2020). What's in a Contribution?. <em>Journal of the Association for Information Systems</em>, 21(1), 2. <a href="https://aisel.aisnet.org/cgi/viewcontent.cgi?article=1928&amp;context=jais" target="_blank">link</a></p>
  <p>O’Brien, B. C., Harris, I. B., Beckman, T. J., Reed, D. A., &amp; Cook, D. A. (2014). Standards for reporting qualitative research: a synthesis of recommendations. <em>Academic Medicine</em>, 89(9), 1245-1251. <a href="https://journals.lww.com/academicmedicine/fulltext/2014/09000/standards_for_reporting_qualitative_researcha.21.aspx" target="_blank">link</a></p>
  <p>Okoli, C. (2015). A guide to conducting a standalone systematic literature review. Communications of the Association for Information Systems, 37. <a href="https://aisel.aisnet.org/cais/vol37/iss1/43/">Link</a></p>
  <p>Page, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., ... &amp; Moher, D. (2021). The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. <em>International Journal of Surgery</em>, 88, 105906. <a href="https://www.sciencedirect.com/science/article/pii/S1743919121000406" target="_blank">link</a></p>
  <p>Paré, G., Trudel, M. C., Jaana, M., &amp; Kitsiou, S. (2015). Synthesizing information systems knowledge: A typology of literature reviews. <em>Information &amp; Management</em>, 52(2), 183-199. <a href="https://www.sciencedirect.com/science/article/pii/S0378720614001116" target="_blank">link</a></p>
  <p>Paré, G., Wagner, G., &amp; Prester, J. (2023). How to develop and frame impactful review articles: key recommendations. <em>Journal of Decision Systems</em>, 1-17. <a href="https://www.tandfonline.com/doi/pdf/10.1080/12460125.2023.2197701" target="_blank">link</a></p>
  <p>Peffers, K., Tuunanen, T., Rothenberger, M. A., &amp; Chatterjee, S. (2007). A design science research methodology for information systems research. <em>Journal of Management Information Systems</em>, 24(3), 45-77. <a href="https://www.tandfonline.com/doi/abs/10.2753/MIS0742-1222240302" target="_blank">link</a></p>
  <p>Prat, N., Comyn-Wattiau, I., &amp; Akoka, J. (2015). A taxonomy of evaluation methods for information systems artifacts. <em>Journal of Management Information Systems</em>, 32(3), 229-267. <a href="https://www.tandfonline.com/doi/abs/10.1080/07421222.2015.1099390" target="_blank">link</a></p>
  <p>Stevens, L. M., Mortazavi, B. J., Deo, R. C., Curtis, L., &amp; Kao, D. P. (2020). Recommendations for reporting machine learning analyses in clinical research. <em>Circulation: Cardiovascular Quality and Outcomes</em>, 13(10), e006556. <a href="https://www.ahajournals.org/doi/full/10.1161/CIRCOUTCOMES.120.006556" target="_blank">Link</a></p>
  <p>Templier, M., &amp; Pare, G. (2018). Transparency in literature reviews: an assessment of reporting practices across review types and genres in top IS journals. <em>European Journal of Information Systems</em>, 27(5), 503-550. <a href="https://www.tandfonline.com/doi/abs/10.1080/0960085X.2017.1398880?casa_token=1V3qftILSxQAAAAA:PuOFx6KxaynRQDZ1Yr07MSzZ_RPNOZiNjHB1zsyq9235rbX5QYv_Vb3NdKQVteywDw53oZ3CwuC9eQ" target="_blank">link</a></p>
  <p>Tong, A., Sainsbury, P., &amp; Craig, J. (2007). Consolidated criteria for reporting qualitative research (COREQ): a 32-item checklist for interviews and focus groups. <em>International Journal for Quality in Health Care</em>, 19(6), 349-357. <a href="https://academic.oup.com/intqhc/article/19/6/349/1791966?login=true" target="_blank">link</a></p>
  <p>Walsh, I., Fishman, D., Garcia-Gasulla, D., Titma, T., Pollastri, G., Harrow, J., ... &amp; Tosatto, S. C. (2021). DOME: recommendations for supervised machine learning validation in biology. <em>Nature Methods</em>, 18(10), 1122-1127. <a href="https://www.nature.com/articles/s41592-021-01205-4" target="_blank">Link</a></p>
</div>
